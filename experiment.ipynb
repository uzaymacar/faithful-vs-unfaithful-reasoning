{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2025-04-05 03:42:44.250554: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743824564.272305  269042 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743824564.279540  269042 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, TypedDict\n",
    "from utils import Question, QsDataset\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 642 faithful and 180 unfaithful CoTs.\n"
     ]
    }
   ],
   "source": [
    "with open('results/deepseek_r1_cots_iphr_eval.json', 'r') as f:\n",
    "    cots = json.load(f)\n",
    "\n",
    "# NOTE: I'm not sure about the below way of getting faithful CoTs\n",
    "# cots_faithful = [x for x in cots if x['is_faithful_iphr'] == True]\n",
    "cots_faithful = [x for x in cots if x['is_faithful_iphr'] == True or (x['is_faithful_iphr'] == None and x['eval_final_answer'] == x['ground_truth_answer'])]\n",
    "cots_unfaithful = [x for x in cots if x['is_faithful_iphr'] == False]\n",
    "print(f\"Found {len(cots_faithful)} faithful and {len(cots_unfaithful)} unfaithful CoTs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "Model loaded: DeepSeek-R1-Distill-Qwen-1.5B\n",
      "  Context length: 2048\n",
      "  Layers: 28\n",
      "  Vocab size: 151936\n",
      "  Hidden dim: 1536\n",
      "  Attention heads: 12\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "# NOTE: Ensure the model name is added to OFFICIAL_MODEL_NAMES in TransformerLens loading_from_pretrained.py. Also consider adjusting n_ctx if needed.\n",
    "model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'\n",
    "context_length = 2048\n",
    "batch_size = 4  # Number of questions to process per batch\n",
    "temperature = 0.6  # Sampling temperature\n",
    "max_new_tokens = 1024  # Maximum number of new tokens to generate\n",
    "top_p = 0.92  # Nucleus sampling top-p value\n",
    "    \n",
    "# Disable gradient computation for faster inference\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Model loading\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "model = HookedTransformer.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "model.cfg.n_ctx = context_length # Adjust context length if necessary\n",
    "print(f\"Model loaded: {model.cfg.model_name}\")\n",
    "print(f\"  Context length: {model.cfg.n_ctx}\")\n",
    "print(f\"  Layers: {model.cfg.n_layers}\")\n",
    "print(f\"  Vocab size: {model.cfg.d_vocab}\")\n",
    "print(f\"  Hidden dim: {model.cfg.d_model}\")\n",
    "print(f\"  Attention heads: {model.cfg.n_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 818, 151936])\n"
     ]
    }
   ],
   "source": [
    "example_unfaithful = cots_unfaithful[0]\n",
    "logits, cache = model.run_with_cache(example_unfaithful['generated_cot'], remove_batch_dim=True)\n",
    "print(logits.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
